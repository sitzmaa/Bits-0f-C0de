---
Id: 1002
Title: Sarcasm Detection Natural Language Processing
Author: Alexander Sitzman
Tags: NLP Python
Topic: NLP
Abstract: A study to determine the effectiveness of different types of language models on sucessful tagging of text as either sarcastic or non-sarcastic
HeaderImage: /BL-1002/neuralnet.jpg
isPublished: true
---

## Intro to Neural Networks

Neural Networks are a type of artificial intelligence model that can infer information from a given text. They have evolved significantly over the years, becoming a cornerstone in various NLP tasks due to their ability to learn complex patterns and representations from large datasets.

### How they work

A Neural Network is a collection of interconnected nodes, each representing a simple equation. These nodes, or neurons, classify inputs based on weights that are adjusted during training.

During training, the network compares its output to a gold standard dataset to measure accuracy. It then propagates errors backward through the network (a process known as backpropagation) to update the weights and improve future predictions. This iterative process enables the network to learn from data. However, the features and weights used by neural networks are typically not interpretable by humans, which can be a limitation when understanding the model's decisions.



## Neural Networks for Natural Language Processing

Neural Networks are particularly well-suited for NLP tasks because they can capture nuances and contextual relationships in text that simpler algorithms might miss. For instance, in tasks such as sentiment analysis, translation, and text generation, neural networks, especially deep learning models like Recurrent Neural Networks (RNNs) and Transformers, have shown remarkable performance.

## Text Classification

Text classification involves tagging text with predefined labels. For NLP models, this task is fundamental yet essential, providing a basis for more complex operations. By using pre-tagged datasets, we can train models and easily verify their accuracy, making it a straightforward entry point into NLP applications.



## Sarcasm Detection

### Project Members

Alexander Sitzman

Chris Farrer

Maxwell Schultz

### Problem Statement

We are attempting to build a set of models which can accurately predict whether a news article title contains sarcasm or not. We are finding ways to effectively normalize and vectorize the data before finding optimal weights to a variety of ML models which will solve the binary classification problem.

### Ethical and Social Impact

This project has the potential to reach a deeper linguistic understanding of subtle pragmatic structures such as sarcasm. It will be able to show the potential of ML machinery paired with NLP methodologies, and how real social phenomena can be modeled in such a way.

### Expected Learning Outcome

We expect to learn more about the reliability of the given dataset, the difficulty of the problem, and the ability of the various models for this type of problem. We expect to better understand the process of setting up various models, and gain deeper insights to the methodology of hyperparameter tweaking.


### Datasets

News Headlines Dataset For Sarcasm Detection (kaggle.com)

A collection of over 28,000 news headlines from real and satirical news outlets. Each datapoint is tagged as either sarcastic or non sarcastic, as well as a link to the original article. The advantage of this data set is that it is  self contained, that is unlike a tweet set, there is not a connection between previous data points to provide context.

Sarcasm detection (kaggle.com)

Another popular dataset from Kaggle is the Sarcasm detection data set. It provides similar examples of headlines tagged by sarcastic and not sarcastic. This data set claims to be smaller and more refined and is newer. May be potentially interesting to see the comparisons between the two datasets

These two cannot both be used at the same time as they may have overlapping data. The strongest approach will be to divide the datasets in half to create a training set and a test set from each one and test the effectiveness of either set independently

### Technical Approach

To achieve the objective of accurately predicting sarcasm in news headlines, we will utilize multiple models to test model efficiency, including both traditional machine learning approaches and neural network approaches. The process will include the following steps:

#### Data Preprocessing

Normalization: Standardizing the text data by converting all headlines to lowercase, removing punctuation, and stemming/lemmatizing words to ensure uniformity.

Vectorization: Transforming text data into numerical vectors using techniques such as TF-IDF (Term Frequency-Inverse Document Frequency) and word embeddings like Word2Vec

#### Model Selection and Training

Traditional Machine Learning Models: We will implement models such as Logistic Regression and Support Vector Machines (SVM) to establish baseline performance metrics.

Neural Networks: We will explore more advanced models like Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs)

#### Model Evaluation

Performance Metrics: Evaluating models using accuracy, precision, recall, and F1 score

#### Comparative Analysis

We will compare the performance of different models to determine the most effective approach for sarcasm detection. Additionally, we will analyze the differences in model performance between the two datasets to gain insights into how dataset characteristics influence model efficacy.


## Our Findings

The project is currently in progress and has not yielded any specific results as of yet

## References

<a href="https://www.kaggle.com/datasets/rmisra/news-headlines-dataset-for-sarcasm-detection">News Headlines Dataset For Sarcasm Detection</a>


<a href="https://www.kaggle.com/datasets/saurabhbagchi/sarcasm-detection-through-nlp">Sarcasm Detection</a>


